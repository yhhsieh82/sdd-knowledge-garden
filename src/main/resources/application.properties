# Query API Runtime Configuration

# Spring AI Ollama Configuration (runtime settings - vary by environment)
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=llama3.2:1b
spring.ai.ollama.chat.options.temperature=0.7
